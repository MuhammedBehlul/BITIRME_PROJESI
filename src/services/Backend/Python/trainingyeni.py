# -*- coding: utf-8 -*-
"""Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1354dyFJseFzWESygULulBnVByxOtmY6T
"""

!pip install fuzzywuzzy[speedup]
!pip install textblob
!pip install keybert
!pip install umap-learn --quiet
from keybert import KeyBERT
import pandas as pd
import numpy as np
import spacy
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import mean_squared_error
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from fuzzywuzzy import process
from scipy.sparse import hstack
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob
import nltk
from sentence_transformers import SentenceTransformer

# Download the WordNet resource for lemmatization
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Load the dataset
users = pd.read_csv("dataset.csv")
users['Interests'].fillna('', inplace=True)
users['About'].fillna('', inplace=True)
users.head()

# Load the English NLP model
nlp = spacy.load("en_core_web_sm")

def lemmatize_text_spacy(text):
    doc = nlp(text)  # Process text using spaCy
    return " ".join([token.lemma_ for token in doc])  # Extract lemmatized words

# Apply lemmatization to 'About' and 'Interests' columns
users['About'] = users['About'].apply(lemmatize_text_spacy)

import re
from keybert import KeyBERT

kw_model = KeyBERT("all-mpnet-base-v2")

# Normalize exclusion set
raw_excluded_keywords = {
    'explore', 'exploring', 'explored', 'exploration', 'exploratory', 'explorer', 'journey', 'wander', 'wandering', 'roam',
    'roaming', 'nomad', 'nomadic', 'venture', 'passion', 'passionate', 'passionately', 'passions', 'impassioned', 'love', 'loving',
    'obsession', 'obsessed', 'enthusiasm', 'enthusiastic', 'devotion', 'devoted', 'zeal', 'zealous', 'eager', 'eagerness', 'craving',
    'desire', 'yearning', 'excitement', 'excited', 'dream', 'dreamer', 'dreaming', 'driven', 'ambition', 'ambitious', 'exciting',
    'life', 'new', 'thing', 'follow', 'happyness', 'inspire', 'getting', 'inspired', 'motivated', 'motivation', 'inspiring', 'play', 'favorite', 'im', 'like',
    'person', 'hobby'

}

def normalize(text):
    return re.sub(r'[^\w\s]', '', text.lower().strip())

# Create normalized exclusion set
excluded_keywords = {normalize(word) for word in raw_excluded_keywords}

def extract_keywords(text, num_keywords=4, max_attempts=5, buffer_multiplier=3):
    if not text:
        return ", ".join([""] * num_keywords)

    attempt = 0
    fallback_keywords = []

    while attempt < max_attempts:
        extract_n = num_keywords * buffer_multiplier
        keywords = kw_model.extract_keywords(
            text,
            keyphrase_ngram_range=(1, 1),
            stop_words='english',
            top_n=extract_n
        )

        fallback_keywords = [kw[0] for kw in keywords]
        filtered_keywords = [kw[0] for kw in keywords if normalize(kw[0]) not in excluded_keywords]

        if len(filtered_keywords) >= num_keywords:
            return ", ".join(filtered_keywords[:num_keywords])

        buffer_multiplier += 1
        attempt += 1

    return ", ".join(fallback_keywords[:num_keywords])

text = "Im a person that loves watching mma with my friends. I like pubs and bars. Fishing is my biggest hobby. "
print(extract_keywords(text))

users['About_Keywords'] = users['About'].apply(lambda x: extract_keywords(x, num_keywords=4))

# Load a pre-trained SBERT model
sbert_model = SentenceTransformer('all-mpnet-base-v2')

# Convert "About_Keywords" to dense embeddings
users['About_Embeddings'] = users['About_Keywords'].apply(lambda x: sbert_model.encode(x))
users['Interests_Embeddings'] = users['Interests'].apply(lambda x: sbert_model.encode(x))
users['Location_Embeddings'] = users['Location'].apply(lambda x: sbert_model.encode(x))
# Convert to a numerical matrix for similarity calculations

vector_about = np.vstack(users['About_Embeddings'].values)
vector_interests = np.vstack(users['Interests_Embeddings'].values)
vector_location = np.vstack(users['Location_Embeddings'].values)

from scipy.sparse import hstack
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(sparse_output=False)  # sparse=False to get a dense array
location_encoded = encoder.fit_transform(users[['Location']])

combined_features = np.hstack([
    vector_about,  # Embeddings from "About_Keywords"
    vector_interests,  # Embeddings from "Interests"
    #users[['Age']].values,  # Numerical feature: Age
    vector_location  # Embeddings from "Location"
])

combined_features = combined_features.astype(np.float64)
similarity = cosine_similarity(combined_features)
similarity

import pandas as pd
import matplotlib.pyplot as plt
import umap.umap_ as umap
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# combined_features ve users dataframe'leri zaten hazır varsayılıyor

# KMeans kümeleme
num_clusters = 24
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
users['cluster'] = kmeans.fit_predict(combined_features)

# Kümeleme kalite metrikleri
labels = kmeans.labels_
silhouette = silhouette_score(combined_features, labels)
davies_bouldin = davies_bouldin_score(combined_features, labels)
calinski_harabasz = calinski_harabasz_score(combined_features, labels)

print(f"Silhouette Score: {silhouette:.4f}")
print(f"Davies-Bouldin Score: {davies_bouldin:.4f}")
print(f"Calinski-Harabasz Score: {calinski_harabasz:.4f}")

# UMAP ile boyut indirgeme
reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)
umap_components = reducer.fit_transform(combined_features)

umap_df = pd.DataFrame(umap_components, columns=['UMAP1', 'UMAP2'])
umap_df['cluster'] = users['cluster']
umap_df['Interests'] = users['Interests']

# UMAP grafiği
plt.figure(figsize=(12, 10))
for cluster in range(num_clusters):
    cluster_data = umap_df[umap_df['cluster'] == cluster]
    interest_counts = cluster_data['Interests'].value_counts()
    most_common_interest = interest_counts.idxmax() if not interest_counts.empty else 'Unknown'
    plt.scatter(cluster_data['UMAP1'], cluster_data['UMAP2'], label=f'Cluster {cluster} - {most_common_interest}', s=60, alpha=0.7)

plt.title('User Clusters Visualized with UMAP')
plt.xlabel('UMAP Component 1')
plt.ylabel('UMAP Component 2')
#plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import umap.umap_ as umap
import hdbscan
import joblib
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# combined_features ve users DataFrame'lerinin hazır olduğunu varsayalım

# 1️⃣ UMAP ile boyut indirgeme (gürültüyü azaltacak parametrelerle)
reducer = umap.UMAP(
    n_neighbors=40,        # artırdım, daha geniş komşuluk
    min_dist=0.05,
    metric='cosine',
    random_state=42
)
umap_components = reducer.fit_transform(combined_features)

# 2️⃣ HDBSCAN ile kümeleme (gürültüyü azaltmak için sıkı ayarlar)
clusterer = hdbscan.HDBSCAN(
    min_cluster_size=8,      # artırdım, daha büyük minimum küme
    min_samples=5,           # artırdım, daha sıkı yoğunluk ölçümü
    metric='euclidean',
    cluster_selection_epsilon=0.1,  # biraz gevşek küme seçimi
    prediction_data=True
)
clusterer.fit(umap_components)
clusters = clusterer.labels_

# 3️⃣ Modelleri kaydetmek için birleştir
model_bundle = {
    'umap': reducer,
    'hdbscan': clusterer
}
#joblib.dump(model_bundle, 'umap_hdbscan_model.pkl')

# 4️⃣ Kullanıcılara küme etiketlerini ekle
users['cluster'] = clusters

# 5️⃣ Gürültü olmayan noktalar için metrik hesapla
mask = clusters != -1
n_noise = sum(clusters == -1)
print(f"Gürültü (Noise) kullanıcı sayısı: {n_noise}")

if len(set(clusters[mask])) > 1:
    silhouette = silhouette_score(umap_components[mask], clusters[mask])
    davies_bouldin = davies_bouldin_score(umap_components[mask], clusters[mask])
    calinski_harabasz = calinski_harabasz_score(umap_components[mask], clusters[mask])

    print(f"Silhouette Score: {silhouette:.4f}")
    print(f"Davies-Bouldin Score: {davies_bouldin:.4f}")
    print(f"Calinski-Harabasz Score: {calinski_harabasz:.4f}")
else:
    print("Yeterli sayıda küme yok, metrikler hesaplanamadı.")

# 6️⃣ Küme görselleştirme
plt.figure(figsize=(12, 10))
unique_clusters = set(clusters)

for cluster in unique_clusters:
    cluster_data = pd.DataFrame(umap_components, columns=['UMAP1', 'UMAP2'])[clusters == cluster]
    cluster_data['Interests'] = users.loc[clusters == cluster, 'Interests'].values

    if cluster == -1:
        label = 'Noise'
        color = 'lightgrey'
    else:
        interest_counts = cluster_data['Interests'].value_counts()
        most_common_interest = interest_counts.idxmax() if not interest_counts.empty else 'Unknown'
        label = f'Cluster {cluster} - {most_common_interest}'
        color = None  # otomatik renk seçimi

    plt.scatter(cluster_data['UMAP1'], cluster_data['UMAP2'], label=label, s=60, alpha=0.7, c=color)

plt.title('User Clusters Visualized with UMAP + HDBSCAN')
plt.xlabel('UMAP Component 1')
plt.ylabel('UMAP Component 2')
plt.grid(True)
plt.tight_layout()
plt.show()

import joblib

# model_bundle dictionary already defined in your training code
joblib.dump(model_bundle, 'umap_hdbscan_modell.pkl')

print("Model saved as 'umap_hdbscan_modell.pkl'")

def recommend_userss(user_id):
    if user_id not in users['ID'].values:
        return ["User not found."]

    # Get the user's index and cluster
    user_idx = users.index[users['ID'] == user_id][0]
    user_cluster = users.loc[user_idx, 'cluster']

    # Get indices of users in the same cluster
    cluster_indices = users[users['cluster'] == user_cluster].index

    # Compute similarity only within the same cluster
    sim_scores = [(i, similarity[user_idx, i]) for i in cluster_indices if i != user_idx]

    # Sort by similarity score (descending) and get the top 20
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[:20]

    # Return all the information of the top 20 similar users
    recommended_users = users.iloc[[i[0] for i in sim_scores]]  # Get the full row data for each recommended user
    return recommended_users

recommend_userss(531)




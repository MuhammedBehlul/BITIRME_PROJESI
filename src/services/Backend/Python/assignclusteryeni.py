# -*- coding: utf-8 -*-
"""assignCluster.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Qc3kJcA-b27Eu6ZkteQY7HLlaqjR510
"""

!pip install firebase-admin sentence-transformers keybert scikit-learn joblib schedule

import firebase_admin
from firebase_admin import credentials, firestore
from sentence_transformers import SentenceTransformer
from keybert import KeyBERT
from sklearn.preprocessing import normalize
import numpy as np
import joblib
import time

# Initialize Firebase
cred = credentials.Certificate("recommendationapp-d3e94-firebase-adminsdk-fbsvc-127bb65217.json")
if not firebase_admin._apps:
    firebase_admin.initialize_app(cred)
db = firestore.client()

# Load models
embedding_model = SentenceTransformer("all-mpnet-base-v2")
kw_model = KeyBERT()
kmeans = joblib.load("trainedModelV2.0.pkl")

# Excluded words from keyword extraction
excluded_keywords = {
    'explore', 'exploring', 'explored', 'exploration', 'exploratory', 'explorer', 'journey',
    'wander', 'wandering', 'roam', 'roaming', 'nomad', 'nomadic', 'venture', 'passion',
    'passionate', 'passionately', 'passions', 'impassioned', 'love', 'loving', 'obsession',
    'obsessed', 'enthusiasm', 'enthusiastic', 'devotion', 'devoted', 'zeal', 'zealous',
    'eager', 'eagerness', 'craving', 'desire', 'yearning', 'excitement', 'excited',
    'dream', 'dreamer', 'dreaming', 'driven', 'ambition', 'ambitious', 'exciting',
    'life', 'new', 'thing'
}

# Extract meaningful keywords
def extract_keywords(text, num_keywords=4, max_attempts=5, buffer_multiplier=3):
    if not text:
        return ""
    attempt = 0
    all_keywords = []
    while attempt < max_attempts:
        extract_n = num_keywords * buffer_multiplier
        keywords = kw_model.extract_keywords(
            text,
            keyphrase_ngram_range=(1, 1),
            stop_words='english',
            top_n=extract_n
        )
        filtered_keywords = [kw[0] for kw in keywords if kw[0].lower() not in excluded_keywords]
        all_keywords = filtered_keywords
        if len(all_keywords) >= num_keywords:
            return ", ".join(all_keywords[:num_keywords])
        buffer_multiplier += 1
        attempt += 1
    return ", ".join(all_keywords[:num_keywords])

# Process a single user document
def process_user(doc_id):
    user_ref = db.collection("users").document(doc_id)
    doc = user_ref.get()
    if not doc.exists:
        return

    data = doc.to_dict()

    try:
        # Extract keywords from introduction
        about_kw = extract_keywords(data.get("introduction", ""))

        # Prepare interests
        raw_interests = data.get("hobbies", "")
        if isinstance(raw_interests, list):
            interests_raw = ", ".join(raw_interests)
        elif isinstance(raw_interests, str):
            interests_raw = raw_interests
        else:
            interests_raw = ""

        # Get location
        location_raw = data.get("city", "")

        # Generate embeddings
        about_emb = embedding_model.encode(about_kw)
        interests_emb = embedding_model.encode(interests_raw)
        location_emb = embedding_model.encode(location_raw)

        # Concatenate and normalize embeddings
        combined = np.concatenate([about_emb, interests_emb, location_emb])
        combined_normalized = normalize([combined])[0]

        # Predict cluster
        cluster = int(kmeans.predict([combined_normalized])[0])

        # Update Firestore document
        user_ref.update({
            "cluster": cluster,
            "keywords": about_kw  # OR .split(", ") for list
        })

        print(f"[✔] Clustered user {doc_id} into cluster {cluster} with keywords: {about_kw}")
    except Exception as e:
        print(f"[✘] Error processing user {doc_id}: {e}")

# Listener for Firestore updates
def on_snapshot(col_snapshot, changes, read_time):
    for change in changes:
        doc_id = change.document.id
        data = change.document.to_dict()

        if change.type.name == "ADDED":
            print(f"[+] Document {doc_id} added.")
            if data.get("cluster") is None:
                process_user(doc_id)

        elif change.type.name == "MODIFIED":
            print(f"[~] Document {doc_id} modified.")
            updated_fields = change.document._data
            relevant_fields = ["introduction", "hobbies", "city"]
            if any(field in updated_fields for field in relevant_fields):
                process_user(doc_id)
            else:
                print(f"[i] No relevant field changed for user {doc_id} — skipping.")

# Start Firestore listener
def start_listener():
    users_ref = db.collection("users")
    users_ref.on_snapshot(on_snapshot)

# Main
if __name__ == "__main__":
    start_listener()
    while True:
        time.sleep(60)

import numpy as np
import time
import firebase_admin
from firebase_admin import credentials, firestore
from sentence_transformers import SentenceTransformer
from keybert import KeyBERT
from sklearn.preprocessing import normalize
import joblib
import hdbscan

# Initialize Firebase
cred = credentials.Certificate("recommendationapp-d3e94-firebase-adminsdk-fbsvc-127bb65217.json")
if not firebase_admin._apps:
    firebase_admin.initialize_app(cred)
db = firestore.client()

# Load models (UMAP + HDBSCAN)
model_bundle = joblib.load("umap_hdbscan_modell.pkl")
umap_model = model_bundle['umap']
hdbscan_model = model_bundle['hdbscan']

embedding_model = SentenceTransformer("all-mpnet-base-v2")
kw_model = KeyBERT()

excluded_keywords = {
    'explore', 'exploring', 'explored', 'exploration', 'exploratory', 'explorer', 'journey',
    'wander', 'wandering', 'roam', 'roaming', 'nomad', 'nomadic', 'venture', 'passion',
    'passionate', 'passionately', 'passions', 'impassioned', 'love', 'loving', 'obsession',
    'obsessed', 'enthusiasm', 'enthusiastic', 'devotion', 'devoted', 'zeal', 'zealous',
    'eager', 'eagerness', 'craving', 'desire', 'yearning', 'excitement', 'excited',
    'dream', 'dreamer', 'dreaming', 'driven', 'ambition', 'ambitious', 'exciting',
    'life', 'new', 'thing'
}

def extract_keywords(text, num_keywords=4, max_attempts=5, buffer_multiplier=3):
    if not text:
        return ""
    attempt = 0
    all_keywords = []
    while attempt < max_attempts:
        extract_n = num_keywords * buffer_multiplier
        keywords = kw_model.extract_keywords(
            text,
            keyphrase_ngram_range=(1, 1),
            stop_words='english',
            top_n=extract_n
        )
        filtered_keywords = [kw[0] for kw in keywords if kw[0].lower() not in excluded_keywords]
        all_keywords = filtered_keywords
        if len(all_keywords) >= num_keywords:
            return ", ".join(all_keywords[:num_keywords])
        buffer_multiplier += 1
        attempt += 1
    return ", ".join(all_keywords[:num_keywords])

def process_user(doc_id):
    user_ref = db.collection("users").document(doc_id)
    doc = user_ref.get()
    if not doc.exists:
        return

    data = doc.to_dict()

    try:
        about_kw = extract_keywords(data.get("introduction", ""))

        raw_interests = data.get("hobbies", "")
        if isinstance(raw_interests, list):
            interests_raw = ", ".join(raw_interests)
        elif isinstance(raw_interests, str):
            interests_raw = raw_interests
        else:
            interests_raw = ""

        location_raw = data.get("city", "")

        about_emb = embedding_model.encode(about_kw)
        interests_emb = embedding_model.encode(interests_raw)
        location_emb = embedding_model.encode(location_raw)

        combined = np.concatenate([about_emb, interests_emb, location_emb])
        combined_normalized = normalize([combined])[0]

        # Burada UMAP ile indirgeme
        umap_vector = umap_model.transform([combined_normalized])

        # HDBSCAN ile tahmin
        labels, strengths = hdbscan.approximate_predict(hdbscan_model, umap_vector)
        cluster = int(labels[0])

        user_ref.update({
            "cluster": cluster,
            "keywords": about_kw
        })

        print(f"[✔] Clustered user {doc_id} into cluster {cluster} with keywords: {about_kw}")
    except Exception as e:
        print(f"[✘] Error processing user {doc_id}: {e}")

def on_snapshot(col_snapshot, changes, read_time):
    for change in changes:
        doc_id = change.document.id
        data = change.document.to_dict()

        if change.type.name == "ADDED":
            print(f"[+] Document {doc_id} added.")
            if data.get("cluster") is None:
                process_user(doc_id)

        elif change.type.name == "MODIFIED":
            print(f"[~] Document {doc_id} modified.")
            updated_fields = change.document._data
            relevant_fields = ["introduction", "hobbies", "city"]
            if any(field in updated_fields for field in relevant_fields):
                process_user(doc_id)
            else:
                print(f"[i] No relevant field changed for user {doc_id} — skipping.")

def start_listener():
    users_ref = db.collection("users")
    users_ref.on_snapshot(on_snapshot)

if __name__ == "__main__":
    start_listener()
    while True:
        time.sleep(60)

